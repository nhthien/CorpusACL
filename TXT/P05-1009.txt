Proceedings of the 43rd Annual Meeting of the ACL, pages 66�74,
Ann Arbor, June 2005. c 2005 Association for Computational Linguistics
Towards Developing Generation Algorithms for Text-to-Text Applications
Radu Soricut and Daniel Marcu
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
 
radu, marcu� @isi.edu
Abstract
We describe a new sentence realization
framework for text-to-text applications.
This framework uses IDL-expressions as
a representation formalism, and a generation mechanism based on algorithms for
intersecting IDL-expressions with probabilistic language models. We present both
theoretical and empirical results concerning the correctness and efficiency of these
algorithms.
1 Introduction
Many of today's most popular natural language applications � Machine Translation, Summarization,
Question Answering � are text-to-text applications.
That is, they produce textual outputs from inputs that
are also textual. Because these applications need
to produce well-formed text, it would appear natural that they are the favorite testbed for generic
generation components developed within the Natural Language Generation (NLG) community. Over
the years, several proposals of generic NLG systems
have been made: Penman (Matthiessen and Bateman, 1991), FUF (Elhadad, 1991), Nitrogen (Knight
and Hatzivassiloglou, 1995), Fergus (Bangalore
and Rambow, 2000), HALogen (Langkilde-Geary,
2002), Amalgam (Corston-Oliver et al., 2002), etc.
Instead of relying on such generic NLG systems,
however, most of the current text-to-text applications use other means to address the generation need.
In Machine Translation, for example, sentences are
produced using application-specific "decoders", inspired by work on speech recognition (Brown et
al., 1993), whereas in Summarization, summaries
are produced as either extracts or using task-specific
strategies (Barzilay, 2003). The main reason for
which text-to-text applications do not usually involve generic NLG systems is that such applications do not have access to the kind of information that the input representation formalisms of current NLG systems require. A machine translation or
summarization system does not usually have access
to deep subject-verb or verb-object relations (such
as ACTOR, AGENT, PATIENT, POSSESSOR, etc.)
as needed by Penman or FUF, or even shallower
syntactic relations (such as subject, object,
premod, etc.) as needed by HALogen.
In this paper, following the recent proposal
made by Nederhof and Satta (2004), we argue
for the use of IDL-expressions as an applicationindependent, information-slim representation language for text-to-text natural language generation.
IDL-expressions are created from strings using four
operators: concatenation (� ), interleave (� ), disjunction (� ), and lock (� ). We claim that the IDL
formalism is appropriate for text-to-text generation,
as it encodes meaning only via words and phrases,
combined using a set of formally defined operators.
Appropriate words and phrases can be, and usually
are, produced by the applications mentioned above.
The IDL operators have been specifically designed
to handle natural constraints such as word choice
and precedence, constructions such as phrasal combination, and underspecifications such as free word
order.
66
CFGs
via intersection with
Deterministic
Non-deterministic
via intersection with
probabilistic LMs
Word/Phrase
based
Fergus, Amalgam
Nitrogen, HALogen
FUF, PENMAN
NLG System
(Nederhof&Satta 2004)
IDL
Representation
(formalism)
Semantic,
few meanings
Syntactically/
Semantically
grounded
Syntactic
dependencies
Representation
(computational)
Linear
Exponential
Linear
Deterministic
Generation
(mechanism)
Non-deterministic
via intersection with
probabilistic LMs
Non-deterministic
via intersection with
probabilistic LMs
(this paper)
IDL
Linear
Generation
(computational)
Optimal Solution
Efficient Run-time
Efficient Run-time
Optimal Solution
Efficient Run-time
All Solutions
Efficient Run-time
Optimal Solution
Linear Linear
based
Word/Phrase
Table 1: Comparison of the present proposal with
current NLG systems.
In Table 1, we present a summary of the representation and generation characteristics of current
NLG systems. We mark by   characteristics that are
needed/desirable in a generation component for textto-text applications, and by � characteristics that
make the proposal inapplicable or problematic. For
instance, as already argued, the representation formalism of all previous proposals except for IDL is
problematic (� ) for text-to-text applications. The
IDL formalism, while applicable to text-to-text applications, has the additional desirable property that
it is a compact representation, while formalisms
such as word-lattices and non-recursive CFGs can
have exponential size in the number of words available for generation (Nederhof and Satta, 2004).
While the IDL representational properties are all
desirable, the generation mechanism proposed for
IDL by Nederhof and Satta (2004) is problematic
(� ), because it does not allow for scoring and
ranking of candidate realizations. Their generation mechanism, while computationally efficient, involves intersection with context free grammars, and
therefore works by excluding all realizations that are
not accepted by a CFG and including (without ranking) all realizations that are accepted.
The approach to generation taken in this paper
is presented in the last row in Table 1, and can be
summarized as a   tiling of generation characteristics of previous proposals (see the shaded area in
Table 1). Our goal is to provide an optimal generation framework for text-to-text applications, in
which the representation formalism, the generation
mechanism, and the computational properties are all
needed and desirable (  ). Toward this goal, we
present a new generation mechanism that intersects
IDL-expressions with probabilistic language models. The generation mechanism implements new algorithms, which cover a wide spectrum of run-time
behaviors (from linear to exponential), depending on
the complexity of the input. We also present theoretical results concerning the correctness and the efficiency input IDL-expression) of our algorithms.
We evaluate these algorithms by performing experiments on a challenging word-ordering task.
These experiments are carried out under a highcomplexity generation scenario: find the most probable sentence realization under an n-gram language
model for IDL-expressions encoding bags-of-words
of size up to 25 (up to 10
���
possible realizations!).
Our evaluation shows that the proposed algorithms
are able to cope well with such orders of complexity, while maintaining high levels of accuracy.
2 The IDL Language for NLG
2.1 IDL-expressions
IDL-expressions have been proposed by Nederhof
& Satta (2004) (henceforth N&S) as a representation for finite languages, and are created from strings
using four operators: concatenation (� ), interleave
(� ), disjunction (� ), and lock (� ). The semantics of
IDL-expressions is given in terms of sets of strings.
The concatenation (� ) operator takes two arguments, and uses the strings encoded by its argument expressions to obtain concatenated strings that
respect the order of the arguments; e.g., � ��� encodes the singleton set ���� . The  nterleave (� )
operator interleaves the strings encoded by its argument expressions; e.g., �� ��!�" encodes the set
��#��$!�%&�'!��( . The ) isjunction (� ) operator allows a choice among the strings encoded by its argument expressions; e.g., �0����1" encodes the set
������ . The 2 ock (� ) operator takes only one argument, and "locks-in" the strings encoded by its
argument expression, such that no additional material can be interleaved; e.g., � �3� �%�1"4!�" encodes
the set ��#��!�� .
Consider the following IDL-expression:
�6587@9%ABA6CD �0 �3FEHG�I �BP@QSRUT#V71I&Q4T%"4 �WFEHG�I ��X494P1ERHYI&T%"�" �
`
I&Q!I �aQ�I#AbIc9(T#Icd1" Fe�"
The concatenation (� ) operator captures precedence
constraints, such as the fact that a determiner like
67
the appears before the noun it determines. The lock
(� ) operator enforces phrase-encoding constraints,
such as the fact that the captives is a phrase which
should be used as a whole. The disjunction (� ) operator allows for multiple word/phrase choice (e.g.,
the prisoners versus the captives), and the interleave (� ) operator allows for word-order freedom,
i.e., word order underspecification at meaning representation level. Among the strings encoded by IDLexpression 1 are the following:
finally the prisoners were released
the captives finally were released
the prisoners were finally released
The following strings, however, are not part of the
language defined by IDL-expression 1:
the finally captives were released
the prisoners were released
finally the captives released were
The first string is disallowed because the � operator locks the phrase the captives. The second string
is not allowed because the � operator requires all its
arguments to be represented. The last string violates
the order imposed by the precedence operator between were and released.
2.2 IDL-graphs
IDL-expressions are a convenient way to compactly represent finite languages. However, IDLexpressions do not directly allow formulations of
algorithms to process them. For this purpose, an
equivalent representation is introduced by N&S,
called IDL-graphs. We refer the interested reader to
the formal definition provided by N&S, and provide
here only an intuitive description of IDL-graphs.
We illustrate in Figure 1 the IDL-graph corresponding to IDL-expression 1. In this graph, vertices  �� and  �� are called initial and final, respectively. Vertices  �� ,   � with in-going
�
-labeled edges,
and  �� ,   � � with out-going  -labeled edges, for example, result from the expansion of the � operator,
while vertices  � ,   with in-going  -labeled edges,
and  � ,  ��  with out-going  -labeled edges result
from the expansion of the � operator. Vertices   �
to  � and  ��� to  ��  result from the expansion of
the two � operators, respectively. These latter vertices are also shown to have rank 1, as opposed to
rank 0 (not shown) assigned to all other vertices.
The ranking of vertices in an IDL-graph is needed
to enforce a higher priority on the processing of the
higher-ranked vertices, such that the desired semantics for the lock operator is preserved.
With each IDL-graph   $" we can associate a finite language: the set of strings that can be generated
by an IDL-specific traversal of   $" , starting from
 �� and ending in  �� . An IDL-expression  and its
corresponding IDL-graph   $" are said to be equivalent because they generate the same finite language,
denoted !  $" .
2.3 IDL-graphs and Finite-State Acceptors
To make the connection with the formulation of our
algorithms, in this section we link the IDL formalism with the more classical formalism of finite-state
acceptors (FSA) (Hopcroft and Ullman, 1979). The
FSA representation can naturally encode precedence
and multiple choice, but it lacks primitives corresponding to the interleave (� ) and lock (� ) operators. As such, an FSA representation must explicitly enumerate all possible interleavings, which are
implicitly captured in an IDL representation. This
correspondence between implicit and explicit interleavings is naturally handled by the notion of a cut
of an IDL-graph   $" .
Intuitively, a cut through   $" is a set of vertices
that can be reached simultaneously when traversing
  $" from the initial node to the final node, following the branches as prescribed by the encoded  , ) ,
and 2 operators, in an attempt to produce a string in
!W $" . More precisely, the initial vertex  �� is considered a cut (Figure 2 (a)). For each vertex in a given
cut, we create a new cut by replacing the start vertex of some edge with the end vertex of that edge,
observing the following rules:
" the vertex that is the start of several edges labeled using the special symbol
�
is replaced
by a sequence of all the end vertices of these
edges (for example,  ��#  � is a cut derived from
 �� (Figure 2 (b))); a mirror rule handles the special symbol  ;
" the vertex that is the start of an edge labeled using vocabulary items or  is replaced by the end
vertex of that edge (for example,  $�%  � ,  ��#   ,
 ��&  � ,  ��& �' are cuts derived from  ��&  � ,  ��#  � ,
68
v1
v0
ve
vs
finally













released
were
captives
prisoners
the
the
v2
1
1
1
1
1
1
1
1
v20
v19
v18
v17
v16
v15
v14
v13
v12
v11
v10
v9
v8
v7
v6
v5
v4
v3
Figure 1: The IDL-graph corresponding to the IDLexpression �6587@9AHA6C1 �  �3FEHG�I �!P@QSRUT#V71I&Q4T"4 �3FEHGI �
Xc9SP1ERBY%IST("�" �
`
I&Q!I �%Q�I#AbIc9(T#Icd1" .
(a)
vs
(c)
v1
finally
v2
v0
vs
(b)
v2
v0
vs
rank 1
rank 0
finally
 v5 the
(e)
v3

v2
v0
vs
the

v2
v0
vs  v6
v1
(d)
v6
v5
v3
Figure 2: Cuts of the IDL-graph in Figure 1 (a-d). A
non-cut is presented in (e).
  �  � , and   �   � , respectively, see Figure 2 (cd)), only if the end vertex is not lower ranked
than any of the vertices already present in the
cut (for example,   �% �' is not a cut that can be
derived from  ��& �' , see Figure 2 (e)).
Note the last part of the second rule, which restricts
the set of cuts by using the ranking mechanism. If
one would allow   �   ' to be a cut, one would imply
that finally may appear inserted between the words
of the locked phrase the prisoners.
We now link the IDL formalism with the FSA formalism by providing a mapping from an IDL-graph
  $" to an acyclic finite-state acceptor
�
 $" . Because both formalisms are used for representing finite languages, they have equivalent representational
power. The IDL representation is much more compact, however, as one can observe by comparing the
IDL-graph in Figure 1 with the equivalent finitestate acceptor
�
 $" in Figure 3. The set of states of
�
 $" is the set of cuts of   $" . The initial state of
the finite-state acceptor is the state corresponding to
cut  �� , and the final states of the finite-state acceptor
are the state corresponding to cuts that contain  �� .
In what follows, we denote a state of
�
 $" by the
name of the cut to which it corresponds. A transiv0v2
vs 
v1v2
v0v4
v0 v10
the
v0v5the v0 v0
v0 v0
v11 v12
v6 v7 v0
v0
v8
v13
prisoners
captives




v10
v1


the
the
v6
v11
prisoners
captives
v5
v1
v1 v1 v1
v1
v7
v12


v1v8
v13
v1
v0v3
v4
v1
finally
finally
finally
v3
v1




v14
v0
v1v9
v0v9
v1v14
finally
finally
finally
finally
ve

v1v15
v0v15
were
were


 
released
released
v16
v16 v17
v17
v18
v18
v19
v19

v20
v1 v1 v1 v1
v0 v0 v0 v0 v0
finally finally finally finally
v20
v1









Figure 3: The finite-state acceptor corresponding to
the IDL-graph in Figure 1.
tion labeled � in
�
 $" between state �  ���������  �� �����  �
and state �   � �
� �����   � �
 �����   � �
  occurs if there is an edge
  � � % � �
 " in   $" . For the example in Figure 3,
the transition labeled were between states �  ��# ��' 
and �  ��&  �  occurs because of the edge labeled were
between nodes   �' and  �� (Figure 1), whereas the
transition labeled finally between states �   �# ��'  and
�   �   �'  occurs because of the edge labeled finally between nodes  �� and  �� (Figure 1). The two representations   $" and
�
 $" are equivalent in the sense
that the language generated by IDL-graph   $" is
the same as the language accepted by FSA
�
 $" .
It is not hard to see that the conversion from the
IDL representation to the FSA representation destroys the compactness property of the IDL formalism, because of the explicit enumeration of all possible interleavings, which causes certain labels to appear repeatedly in transitions. For example, a transition labeled finally appears 11 times in the finitestate acceptor in Figure 3, whereas an edge labeled
finally appears only once in the IDL-graph in Figure 1.
3 Computational Properties of
IDL-expressions
3.1 IDL-graphs and Weighted Finite-State
Acceptors
As mentioned in Section 1, the generation mechanism we propose performs an intersection of IDLexpressions with n-gram language models. Following (Mohri et al., 2002; Knight and Graehl, 1998),
we implement language models using weighted
finite-state acceptors (wFSA). In Section 2.3, we
presented a mapping from an IDL-graph   $" to a
finite-state acceptor
�
 $" . From such a finite-state
acceptor
�
 $" , we arrive at a weighted finite-state
acceptor   $" , by splitting the states of
�
 $" ac69
cording to the information needed by the language
model to assign weights to transitions. For example, under a bigram language model !�  , state
�  ��  ��'  in Figure 3 must be split into three different states, � P1Q4RBT&V7DI&Q4T %  �   �'  , �bXc9SP1ERBY%IST %  �   �'  , and
�587@9%ABA6C@% ��  ��'  , according to which (non-epsilon)
transition was last used to reach this state. The
transitions leaving these states have the same labels as those leaving state �  $�  ��'  , and are now
weighted using the language model probability distributions �����  ���P@Q4RBT&V%71I&Q4T%" , �����  ��� Xc94P1ERHYI&T" , and
� ���  ���587@9AHA C " , respectively.
Note that, at this point, we already have a na�
ive
algorithm for intersecting IDL-expressions with ngram language models. From an IDL-expression  ,
following the mapping    $"
�
 $"
  $" , we arrive at a weighted finite-state acceptor, on which we can use a single-source shortestpath algorithm for directed acyclic graphs (Cormen
et al., 2001) to extract the realization corresponding
to the most probable path. The problem with this algorithm, however, is that the premature unfolding of
the IDL-graph into a finite-state acceptor destroys
the representation compactness of the IDL representation. For this reason, we devise algorithms
that, although similar in spirit with the single-source
shortest-path algorithm for directed acyclic graphs,
perform on-the-fly unfolding of the IDL-graph, with
a mechanism to control the unfolding based on the
scores of the paths already unfolded. Such an approach has the advantage that prefixes that are extremely unlikely under the language model may be
regarded as not so promising, and parts of the IDLexpression that contain them may not be unfolded,
leading to significant savings.
3.2 Generation via Intersection of
IDL-expressions with Language Models
Algorithm IDL-NGLM-BFS The first algorithm
that we propose is algorithm IDL-NGLM-BFS in
Figure 4. The algorithm builds a weighted finitestate acceptor  corresponding to an IDL-graph
 incrementally, by keeping track of a set of active states, called 9 X�ERBY%I . The incrementality comes
from creating new transitions and states in  originating in these active states, by unfolding the IDLgraph  ; the set of newly unfolded states is called
 7#VA d . The new transitions in  are weighted acIDL-NGLM-BFS    !  "
1 9 X�ERBY%I ���   "!  
2 #09$% e
3 while # 9$
4 do  7#VA d& UNFOLDIDLG�9 X ERHYI  a"
5 EVALUATENGLM 
 7#VAbd !�  "
6 if FINALIDLG
 7#VAbd  "
7 then #09$%('
8 9 X�ERBY%I)  7&V%A d
9 return 9 X�ERBY%I
Figure 4: Pseudo-code for intersecting an IDL-graph
 with an n-gram language model !  using incremental unfolding and breadth-first search.
cording to the language model. If a final state of
 is not yet reached, the while loop is closed by
making the  7&V%A d set of states to be the next set of
9 X�ERBY%I states. Note that this is actually a breadthfirst search (BFS) with incremental unfolding. This
algorithm still unfolds the IDL-graph completely,
and therefore suffers from the same drawback as the
na�
ive algorithm.
The interesting contribution of algorithm
IDL-NGLM-BFS, however, is the incremental
unfolding. If, instead of line 8 in Figure 4, we
introduce mechanisms to control which  7&V%A d
states become part of the 9 X ERHYI state set for the
next unfolding iteration, we obtain a series of more
effective algorithms.
Algorithm IDL-NGLM-A0 We arrive at algorithm IDL-NGLM-A0 by modifying line 8 in Figure 4, thus obtaining the algorithm in Figure 5. We
use as control mechanism a priority queue, 9(T&E 9Q"1 ,
in which the states from  72&VAbd are PUSH-ed, sorted
according to an admissible heuristic function (Russell and Norvig, 1995). In the next iteration, 9 X ERHYI
is a singleton set containing the state POP-ed out
from the top of the priority queue.
Algorithm IDL-NGLM-BEAM We arrive at algorithm IDL-NGLM-BEAM by again modifying
line 8 in Figure 4, thus obtaining the algorithm in
Figure 6. We control the unfolding using a probabilistic beam 34I!954 , which, via the BEAMSTATES
function, selects as 9 X ERHYI states only the states in
70
IDL-NGLM-A0    !  "
1 9 X�ERBY%I ���   "!  
2 #09$% e
3 while # 9$
4 do  7#VA d& UNFOLDIDLG�9 X ERHYI  a"
5 EVALUATENGLM 
 7#VAbd !�  "
6 if FINALIDLG
 7#VAbd  "
7 then #09$%('
8 for each TSE 9%E I in  7#VAbd
do PUSH �9(T&E 9Q51 &TSE 9%E I%"
9 X ERHYI) POP �9TSE 9Q51 "
9 return 9 X�ERBY%I
Figure 5: Pseudo-code for intersecting an IDL-graph
 with an n-gram language model !  using incremental unfolding and A0 search.
IDL-NGLM-BEAM    !   34I!954 "
1 9 X�ERBY%I ���   "!  
2 #09$% e
3 while # 9$
4 do  7#VA d& UNFOLDIDLG�9 X ERHYI  a"
5 EVALUATENGLM 
 7#VAbd !�  "
6 if FINALIDLG
 7#VAbd  "
7 then #09$%('
8 9 X�ERBY%I) BEAMSTATES 
 7#VA d 3cIc954 "
9 return 9 X�ERBY%I
Figure 6: Pseudo-code for intersecting an IDL-graph
 with an n-gram language model !  using incremental unfolding and probabilistic beam search.
 7#VA d reachable with a probability higher or equal
to the current maximum probability times the probability beam 3cIc954 .
3.3 Computing Admissible Heuristics for
IDL-expressions
The IDL representation is ideally suited for computing accurate admissible heuristics under language models. These heuristics are needed by the
IDL-NGLM-A0 algorithm, and are also employed
for pruning by the IDL-NGLM-BEAM algorithm.
For each state
�
in a weighted finite-state acceptor  corresponding to an IDL-graph  , one can
efficiently extract from  � without further unfolding � the set1 of all edge labels that can be used to
reach the final states of  . This set of labels, denoted ���������
 , is an overestimation of the set of future events reachable from
�
, because the labels under the � operators are all considered. From ��� ����

and the  -1 labels (when using an  -gram language
model) recorded in state
�
we obtain the set of label
sequences of length  -1. This set, denoted ��  , is
an (over)estimated set of possible future conditioning events for state
�
, guaranteed to contain the most
cost-efficient future conditioning events for state
�
.
Using ��  , one needs to extract from �� ���
 the
set of most cost-efficient future events from under
each � operator. We use this set, denoted ���  , to
arrive at an admissible heuristic for state
�
under a
language model !�  , using Equation 2:


�
"!#" �%$'&)(1032547698 A@ �CB
D �%$'&FEC(0 � � � HG �IPG"�" (2)
If

0 
�
" is the true future cost for state
�
, we guarantee that


�
"RQ

0%
�
" from the way ���  and
��  are constructed. Note that, as it usually happens with admissible heuristics, we can make


�
"
come arbitrarily close to

0%
�
" , by computing increasingly better approximations ��  of �� 0 .
Such approximations, however, require increasingly
advanced unfoldings of the IDL-graph  (a complete unfolding of  for state
�
gives �� 
�� 0 , and consequently


�
"S

0%
�
" ). It follows that arbitrarily accurate admissible heuristics
exist for IDL-expressions, but computing them onthe-fly requires finding a balance between the time
and space requirements for computing better heuristics and the speed-up obtained by using them in the
search algorithms.
3.4 Formal Properties of IDL-NGLM
algorithms
The following theorem states the correctness of our
algorithms, in the sense that they find the maximum
probability path encoded by an IDL-graph under an
n-gram language model.
Theorem 1 Let  be an IDL-expression, G( )
its IDL-graph, and W( ) its wFSA under
an n-gram language model LM. Algorithms
IDL-NGLM-BFS and IDL-NGLM-A0 find the
1
Actually, these are multisets, as we treat multiply-occurring
labels as separate items.
71
path of maximum probability under LM. Algorithm
IDL-NGLM-BEAM finds the path of maximum
probability under LM, if all states in W( ) along
this path are selected by its BEAMSTATES function.
The proof of the theorem follows directly from the
correctness of the BFS and A0 search, and from the
condition imposed on the beam search.
The next theorem characterizes the run-time complexity of these algorithms, in terms of an input IDLexpression  and its corresponding IDL-graph   $"
complexity. There are three factors that linearly influence the run-time complexity of our algorithms:
  is the maximum number of nodes in   $" needed
to represent a state in
�
 $" �   depends solely on  ;
� is the maximum number of nodes in   $" needed
to represent a state in   $" � � depends on  and
 , the length of the context used by the  -gram language model; and
�
is the number of states of   $"
�
�
also depends on  and  . Of these three factors,
�
is by far the predominant one, and we simply call
�
the complexity of an IDL-expression.
Theorem 2 Let  be an IDL-expression,   $" its
IDL-graph,
�
 $" its FSA, and   $" its wFSA
under an n-gram language model. Let � �
�
 $" 
be the set of states of
�
 $" , and � �   $" 
the set of states of   $" . Let also  
@ �CB D $���� ��� �I �, � @ �CB D $�� �� �I �, and
�
� � �   $"  �. Algorithms IDL-NGLM-BFS
and IDL-NGLM-BEAM have run-time complexity
!

 "� �
" . Algorithm IDL-NGLM-A0 has run-time
complexity
!

 "� �
4 698
�
" .
We omit the proof here due to space constraints. The
fact that the run-time behavior of our algorithms is
linear in the complexity of the input IDL-expression
(with an additional log factor in the case of A0
search due to priority queue management) allows us
to say that our algorithms are efficient with respect
to the task they accomplish.
We note here, however, that depending on the
input IDL-expression, the task addressed can vary
in complexity from linear to exponential. That
is, for the intersection of an IDL-expression 
�
� �  ����� 
�$# " (bag of  words) with a trigram language model, we have    $"�  , �  $" &%(' ,
�
I
#
I0) e , and therefore a
!
 
�
I
#
" complexity. This exponential complexity comes as no
surprise given that the problem of intersecting an ngram language model with a bag of words is known
to be NP-complete (Knight, 1999). On the other
hand, for intersecting an IDL-expression  � � �
����� �
� # (sequence of  words) with a trigram language model, we have    $" e , �  $" 21 , and
�
 , and therefore an
!
 '" generation algorithm.
In general, for IDL-expressions for which   is
bounded, which we expect to be the case for most
practical problems, our algorithms perform in polynomial time in the number of words available for
generation.
4 Evaluation of IDL-NGLM Algorithms
In this section, we present results concerning
the performance of our algorithms on a wordordering task. This task can be easily defined as
follows: from a bag of words originating from
some sentence, reconstruct the original sentence as
faithfully as possible. In our case, from an original
sentence such as "the gifts are donated by american companies", we create the IDL-expression 3 �4 �
�FEHG�I  $%R &EHT�d V7@9%E Icd1�XcV54WP9 7 R IST%"3&C@�9QcI �9 4 I&QSRX49%7 " �
3655�4 , from which some algorithm realizes a sentence such as "donated by the american companies
are gifts". Note the natural way we represent in
an IDL-expression beginning and end of sentence
constraints, using the � operator. Since this is
generation from bag-of-words, the task is known to
be at the high-complexity extreme of the run-time
behavior of our algorithms. As such, we consider it
a good test for the ability of our algorithms to scale
up to increasingly complex inputs.
We use a state-of-the-art, publicly available
toolkit2 to train a trigram language model using
Kneser-Ney smoothing, on 10 million sentences
(170 million words) from the Wall Street Journal
(WSJ), lower case and no final punctuation. The test
data is also lower case (such that upper-case words
cannot be hypothesized as first words), with final
punctuation removed (such that periods cannot be
hypothesized as final words), and consists of 2000
unseen WSJ sentences of length 3-7, and 2000 unseen WSJ sentences of length 10-25.
The algorithms we tested in this experiments were
the ones presented in Section 3.2, plus two baseline
algorithms. The first baseline algorithm, L, uses an
2
http://www.speech.sri.com/projects/srilm/
72
inverse-lexicographic order for the bag items as its
output, in order to get the word the on sentence initial position. The second baseline algorithm, G, is
a greedy algorithm that realizes sentences by maximizing the probability of joining any two word sequences until only one sequence is left.
For the A0 algorithm, an admissible cost is computed for each state
�
in a weighted finite-state automaton, as the sum (over all unused words) of the
minimum language model cost (i.e., maximum probability) of each unused word when conditioning over
all sequences of two words available at that particular state for future conditioning (see Equation 2, with
���  ��������
 ). These estimates are also used by
the beam algorithm for deciding which IDL-graph
nodes are not unfolded. We also test a greedy version of the A0 algorithm, denoted A0 , which considers for unfolding only the nodes extracted from
the priority queue which already unfolded a path of
length greater than or equal to the maximum length
already unfolded minus   (in this notation, the A0
algorithm would be denoted A0� ). For the beam algorithms, we use the notation B� to specify a probabilistic beam of size � , i.e., an algorithm that beams
out the states reachable with probability less than the
current maximum probability times � .
Our first batch of experiments concerns bags-ofwords of size 3-7, for which exhaustive search is
possible. In Table 2, we present the results on the
word-ordering task achieved by various algorithms.
We evaluate accuracy performance using two automatic metrics: an identity metric, ID, which measures the percent of sentences recreated exactly, and
BLEU (Papineni et al., 2002), which gives the geometric average of the number of uni-, bi-, tri-, and
four-grams recreated exactly. We evaluate the search
performance by the percent of Search Errors made
by our algorithms, as well as a percent figure of Estimated Search Errors, computed as the percent of
searches that result in a string with a lower probability than the probability of the original sentence.
To measure the impact of using IDL-expressions for
this task, we also measure the percent of unfolding
of an IDL graph with respect to a full unfolding. We
report speed results as the average number of seconds per bag-of-words, when using a 3.0GHz CPU
machine under a Linux OS.
The first notable result in Table 2 is the savings
ALG ID BLEU Search Unfold Speed
(%) Errors (%) (%) (sec./bag)
L 2.5 9.5 97.2 (95.8) N/A .000
G 30.9 51.0 67.5 (57.6) N/A .000
BFS 67.1 79.2 0.0 (0.0) 100.0 .072
A0 67.1 79.2 0.0 (0.0) 12.0 .010
A0� 60.5 74.8 21.1 (11.9) 3.2 .004
A0� 64.3 77.2 8.5 (4.0) 5.3 .005
B��� � 65.0 78.0 9.2 (5.0) 7.2 .006
B��� � 66.6 78.8 3.2 (1.7) 13.2 .011
Table 2: Bags-of-words of size 3-7: accuracy (ID,
BLEU), Search Errors (and Estimated Search Errors), space
savings (Unfold), and speed results.
achieved by the A0 algorithm under the IDL representation. At no cost in accuracy, it unfolds only
12% of the edges, and achieves a 7 times speedup, compared to the BFS algorithm. The savings
achieved by not unfolding are especially important,
since the exponential complexity of the problem is
hidden by the IDL representation via the folding
mechanism of the � operator. The algorithms that
find sub-optimal solutions also perform well. While
maintaining high accuracy, the A0� and B��� � algorithms unfold only about 5-7% of the edges, at 12-14
times speed-up.
Our second batch of experiments concerns bagof-words of size 10-25, for which exhaustive search
is no longer possible (Table 3). Not only exhaustive
search, but also full A0 search is too expensive in
terms of memory (we were limited to 2GiB of RAM
for our experiments) and speed. Only the greedy
versions A0� and A0� , and the beam search using tight
probability beams (0.2-0.1) scale up to these bag
sizes. Because we no longer have access to the string
of maximum probability, we report only the percent of Estimated Search Errors. Note that, in terms
of accuracy, we get around 20% Estimated Search
Errors for the best performing algorithms (A0� and
B��� � ), which means that 80% of the time the algorithms are able to find sentences of equal or better
probability than the original sentences.
5 Conclusions
In this paper, we advocate that IDL expressions
can provide an adequate framework for develop73
ALG ID BLEU Est. Search Speed
(%) Errors (%) (sec./bag)
L 0.0 1.4 99.9 0.0
G 1.2 31.6 83.6 0.0
A0� 5.8 47.7 34.0 0.7
A0� 7.4 51.2 21.4 9.5
B��� � 9.0 52.1 23.3 7.1
B��� � 12.2 52.6 19.9 36.7
Table 3: Bags-of-words of size 10-25: accuracy (ID,
BLEU), Estimated Search Errors, and speed results.
ing text-to-text generation capabilities. Our contribution concerns a new generation mechanism that
implements intersection between an IDL expression
and a probabilistic language model. The IDL formalism is ideally suited for our approach, due to
its efficient representation and, as we show in this
paper, efficient algorithms for intersecting, scoring,
and ranking sentence realizations using probabilistic
language models.
We present theoretical results concerning the correctness and efficiency of the proposed algorithms,
and also present empirical results that show that
our algorithms scale up to handling IDL-expressions
of high complexity. Real-world text-to-text generation tasks, such as headline generation and machine
translation, are likely to be handled graciously in this
framework, as the complexity of IDL-expressions
for these tasks tends to be lower than the complexity of the IDL-expressions we worked with in our
experiments.
Acknowledgment
This work was supported by DARPA-ITO grant
NN66001-00-1-9814.
References
Srinivas Bangalore and Owen Rambow. 2000. Using
TAG, a tree model, and a language model for generation. In Proceedings of the 1st International Natural
Language Generation Conference.
Regina Barzilay. 2003. Information Fusion for Multidocument Summarization: Paraphrasing and Generation. Ph.D. thesis, Columbia University.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263�311.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2001. Introduction to Algorithms. The MIT Press and McGraw-Hill. Second
Edition.
Simon Corston-Oliver, Michael Gamon, Eric K. Ringger,
and Robert Moore. 2002. An overview of Amalgam:
A machine-learned generation module. In Proceedings of the International Natural Language Generation Conference.
Michael Elhadad. 1991. FUF User manual -- version
5.0. Technical Report CUCS-038-91, Department of
Computer Science, Columbia University.
John E. Hopcroft and Jeffrey D. Ullman. 1979. Introduction to automata theory, languages, and computation.
Addison-Wesley.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599�
612.
Kevin Knight and Vasileios Hatzivassiloglou. 1995. Two
level, many-path generation. In Proceedings of the Association of Computational Linguistics.
Kevin Knight. 1999. Decoding complexity in wordreplacement translation models. Computational Linguistics, 25(4):607�615.
Irene Langkilde-Geary. 2002. A foundation for generalpurpose natural language generation: sentence realization using probabilistic models of language. Ph.D.
thesis, University of Southern California.
Christian Matthiessen and John Bateman. 1991. Text
Generation and Systemic-Functional Linguistic. Pinter Publishers, London.
Mehryar Mohri, Fernando Pereira, and Michael Riley. 2002. Weighted finite-state transducers in
speech recognition. Computer Speech and Language,
16(1):69�88.
Mark-Jan Nederhof and Giorgio Satta. 2004. IDLexpressions: a formalism for representing and parsing
finite languages in natural language processing. Journal of Artificial Intelligence Research, 21:287�317.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the Association for Computational Linguistics (ACL-2002),
pages 311�318, Philadelphia, PA, July 7-12.
Stuart Russell and Peter Norvig. 1995. Artificial Intelligence. A Modern Approach. Prentice Hall, Englewood
Cliffs, New Jersey.
74

